print("Tibble con transformación aplicada:")
print(datos)
# Añadir fila
nueva_fila <- tibble(
ID = 5,
nombre = "Luis",
edad = 29,
ciudad = "Málaga"
)
# Insertar fila
datos <- bind_rows(datos, nueva_fila)
# Mostrar
print("Tibble con nueva fila añadida:")
print(datos)
library(tidyr)
# Datos
temperaturas <- data.frame(
dia_semana = c("lunes", "martes", "miércoles"),
hora_0 = c(20, 22, 19),
hora_1 = c(18, 21, 20),
hora_2 = c(17, 20, 19)
)
temperaturas
# Una fila por dia, hora y temperatura
temperaturas_reorganizadas <- temperaturas %>%
pivot_longer(cols = starts_with("hora"),
names_to = "hora",
values_to = "temperatura")
print(temperaturas_reorganizadas)#lunes      hora_0          20
# tidyverse
if (!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
# Cargar conjunto de datos
data(mtcars)
mtcars
# Dimensión
print(dim(mtcars))
# Columnas
print(length(mtcars))
# Filas
print(nrow(mtcars))
# Columnas
print(ncol(mtcars))
# Filtrar solo los que tienen más de 150 caballos
mtcars_filtrado <- mtcars %>%
filter(hp > 150)
# Dimensión
print(dim(mtcars_filtrado))
# Resumen de los datos, agrupándolos
resumen_estadistico <- mtcars_filtrado %>%
summarise(media_hp = mean(hp),
mediana_mpg = median(mpg),
maximo_disp = max(disp))
# Ver relación entre hp y mpg
grafico <- ggplot(mtcars_filtrado, aes(x = hp, y = mpg)) +
geom_point() +
geom_smooth(method = "lm") +
labs(x = "Caballos de Fuerza", y = "Millas por Galón", title = "Relación entre HP y MPG")
# Ver resumen y gráfico
print(resumen_estadistico)
print(grafico)
setwd("C:\\Users\\jmlozanoo\\Documents\\GitHub\\BIB-R\\AAALIMPIO")
# Cargar librerías necesarias
library(tidyverse)
library(readr)
# Cargar los datos
telco_data <- read_csv("telco-customer-churn.csv")
# Resumen y descripción de los datos
summary(telco_data)
str(telco_data)
# Contar el número de filas con valores nulos
total_na_rows <- sum(!complete.cases(telco_data))
cat("Número total de filas con valores nulos:", total_na_rows, "\n")
# Contar el número de valores nulos por atributo
na_count <- colSums(is.na(telco_data))
cat("Número de valores nulos por atributo:\n")
print(na_count)
# Manejar valores nulos
# Reemplazar nulos en 'gender' con la moda
mode_gender <- as.character(names(sort(table(telco_data$gender), decreasing = TRUE)[1]))
telco_data$gender[is.na(telco_data$gender)] <- mode_gender
# Reemplazar nulos en 'MonthlyCharges' con la media
mean_monthly_charges <- mean(telco_data$MonthlyCharges, na.rm = TRUE)
telco_data$MonthlyCharges[is.na(telco_data$MonthlyCharges)] <- mean_monthly_charges
# Eliminar filas con nulos en 'Contract'
telco_data <- telco_data[!is.na(telco_data$Contract), ]
# Eliminar filas con nulos en 'TotalCharges'
telco_data <- telco_data[!is.na(telco_data$TotalCharges), ]
# Contar el número de filas con valores nulos
total_na_rows <- sum(!complete.cases(telco_data))
cat("Número total de filas con valores nulos:", total_na_rows, "\n")
# Gráfico de caja y bigotes de TotalCharges separado por Contract
ggplot(telco_data, aes(x = Contract, y = TotalCharges)) +
geom_boxplot() +
labs(title = "Boxplot of TotalCharges by Contract",
x = "Contract",
y = "TotalCharges")
# Gráfico de caja y bigotes de TotalCharges con facet grid por Contract
ggplot(telco_data, aes(x = "", y = TotalCharges)) +
geom_boxplot() +
facet_grid(. ~ Contract) +
labs(title = "Boxplot of TotalCharges Faceted by Contract",
x = "",
y = "TotalCharges")
library(tidyverse)
# Crear un gráfico de barras de InternetService y el acumulado de MonthlyCharges
ggplot(telco_data, aes(x = InternetService, y = MonthlyCharges, fill = InternetService)) +
geom_bar(stat = "identity") +
labs(title = "Total Monthly Charges by Internet Service",
x = "Internet Service",
y = "Total Monthly Charges") +
theme_minimal()
# Calcular el valor medio de MonthlyCharges por Contract
mean_monthly_charges_by_contract <- telco_data %>%
group_by(Contract) %>%
summarise(mean_monthly_charges = mean(MonthlyCharges, na.rm = TRUE))
# Mostrar la tabla
print(mean_monthly_charges_by_contract)
# Contar clientes por InternetService y OnlineSecurity
count_by_internet_and_security <- telco_data %>%
group_by(InternetService, OnlineSecurity) %>%
summarise(count = n())
# Mostrar la tabla
print(count_by_internet_and_security)
# Histograma de ternure
ggplot(telco_data, aes(x = tenure)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black") +
labs(title = "Distribution of Tenure",
x = "Tenure (months)",
y = "Number of Customers") +
theme_minimal()
# Gráfico de barras de payment method
ggplot(telco_data, aes(x = PaymentMethod, fill = PaymentMethod)) +
geom_bar() +
labs(title = "Payment Method Distribution",
x = "Payment Method",
y = "Number of Customers") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Gráfico de Densidad de MonthlyCharges por Churn
ggplot(telco_data, aes(x = MonthlyCharges, fill = Churn)) +
geom_density(alpha = 0.5) +
labs(title = "Density Plot of Monthly Charges by Churn",
x = "Monthly Charges",
y = "Density") +
theme_minimal()
# Gráfico de Barras Apiladas de Contract y Churn
ggplot(telco_data, aes(x = Contract, fill = Churn)) +
geom_bar(position = "fill") +
labs(title = "Churn Rate by Contract Type",
x = "Contract Type",
y = "Proportion of Customers") +
theme_minimal()
# Scatter Plot de TotalCharges vs tenure coloreado por Churn
ggplot(telco_data, aes(x = tenure, y = TotalCharges, color = Churn)) +
geom_point(alpha = 0.6) +
labs(title = "Total Charges vs Tenure by Churn",
x = "Tenure (months)",
y = "Total Charges") +
theme_minimal()
library(reshape2)
install.packages("ggcorrplot")
library(ggcorrplot)
library(dplyr)
# Seleccionar solo las columnas numéricas
numeric_data <- telco_data %>% select(tenure, MonthlyCharges, TotalCharges)
# Calcular la matriz de correlación
cor_matrix <- cor(numeric_data, use = "complete.obs")
# Crear el heatmap
ggcorrplot(cor_matrix, method = "circle", type = "lower", lab = TRUE, lab_size = 3,
title = "Correlation Matrix of Numeric Variables",
ggtheme = theme_minimal())
# Gráfico de Barras de SeniorCitizen y Churn
ggplot(telco_data, aes(x = factor(SeniorCitizen), fill = Churn)) +
geom_bar(position = "fill") +
labs(title = "Churn Rate by Senior Citizen Status",
x = "Senior Citizen",
y = "Proportion of Customers",
fill = "Churn") +
scale_x_discrete(labels = c("No", "Yes")) +
theme_minimal()
install.packages("GGally")
library(GGally)
# Seleccionar las variables numéricas y la variable Churn
numeric_data_with_churn <- telco_data %>%
select(tenure, MonthlyCharges, TotalCharges, Churn)
# Crear el pairplot
ggpairs(numeric_data_with_churn, aes(color = Churn, alpha = 0.5),
title = "Pairplot of Numeric Variables by Churn",
columns = 1:3) +
theme_minimal()
# Seleccionar las variables numéricas y la variable Churn
numeric_data_with_churn <- telco_data %>%
select(tenure, MonthlyCharges, TotalCharges, Churn)
library(GGally)
# Seleccionar las variables numéricas y la variable Churn
numeric_data_with_churn <- telco_data %>%
select(tenure, MonthlyCharges, TotalCharges, Churn)
library(dplyr)
# Seleccionar las variables numéricas y la variable Churn
numeric_data_with_churn <- telco_data %>%
select(tenure, MonthlyCharges, TotalCharges, Churn)
# Crear el pairplot
ggpairs(numeric_data_with_churn, aes(color = Churn, alpha = 0.5),
title = "Pairplot of Numeric Variables by Churn",
columns = 1:3) +
theme_minimal()
# Calcular el porcentaje de clientes por cada tipo de cliente, método de pago y servicio de Internet
percentage_by_group <- telco_data %>%
group_by(SeniorCitizen, PaymentMethod, InternetService) %>%
summarise(count = n()) %>%
mutate(percentage = count / sum(count) * 100)
# Mostrar la tabla con los porcentajes
print(percentage_by_group)
# Calcular el porcentaje de clientes que Churned (dejaron la empresa)
churn_percentage <- telco_data %>%
summarise(churn_rate = mean(Churn == "Yes") * 100)
# Mostrar el resultado
print(churn_percentage)
# Comparar cargos mensuales promedio por tipo de contrato
monthly_charges_summary <- telco_data %>%
group_by(Contract) %>%
summarise(mean_monthly_charges = mean(MonthlyCharges, na.rm = TRUE))
# Mostrar el resultado
print(monthly_charges_summary)
# Contar clientes por tipo de InternetService y OnlineSecurity
count_by_internet_security <- telco_data %>%
filter(!is.na(InternetService) & !is.na(OnlineSecurity)) %>%
count(InternetService, OnlineSecurity)
# Mostrar el resultado
print(count_by_internet_security)
# Calcular el número total de clientes
total_clients <- nrow(telco_data)
total_clients
# Calcular el porcentaje de clientes por género y contrato
percentage_by_gender_contract <- telco_data %>%
group_by(gender, Contract) %>%
summarise(count = n()) %>%
mutate(percentage = (count / total_clients) * 100)
# Mostrar el resultado
print(percentage_by_gender_contract)
# Crear los intervalos (bins) para TotalCharges
breaks <- seq(0, max(telco_data$TotalCharges, na.rm = TRUE), by = 500)
totalcharges_bins <- cut(telco_data$TotalCharges, breaks = breaks, include.lowest = TRUE)
# Crear la tabla de frecuencia
totalcharges_freq_table <- table(totalcharges_bins)
# Convertir la tabla de frecuencia a un data frame para una visualización más fácil
totalcharges_freq_df <- as.data.frame(totalcharges_freq_table)
names(totalcharges_freq_df) <- c("Interval", "Frequency")
# Imprimir la tabla de frecuencia
print(totalcharges_freq_df)
# Crear un histograma de TotalCharges
ggplot(telco_data, aes(x = TotalCharges)) +
geom_histogram(binwidth = 500, fill = "blue", color = "black", alpha = 0.7) +
labs(title = "Distribución de TotalCharges", x = "TotalCharges", y = "Frecuencia") +
theme_minimal()
# Filtrar por contrato "Month-to-month" y servicio de Internet "Fiber optic"
filtered_data <- telco_data %>%
filter(Contract == "Month-to-month", InternetService == "Fiber optic")
filtered_data
# Encontrar el customerID con el mayor TotalCharges
customer_max_totalcharges <- filtered_data %>%
top_n(1, TotalCharges) %>%
select(customerID)
# Imprimir el customerID
print(customer_max_totalcharges)
# Instalar las bibliotecas necesarias
install.packages(c("tidyverse", "caret", "e1071", "randomForest", "class", "rpart", "rpart.plot"))
# Cargar las bibliotecas
library(tidyverse)
library(caret)
library(e1071)
library(randomForest)
library(class)
library(rpart)
library(rpart.plot)
# Cargar los datos
telco_data <- read_csv("telco-customer-churn.csv")
# Convertir las variables categóricas en factores
telco_data <- telco_data %>%
mutate(
gender = as.factor(gender),
SeniorCitizen = as.factor(SeniorCitizen),
Partner = as.factor(Partner),
Dependents = as.factor(Dependents),
PhoneService = as.factor(PhoneService),
MultipleLines = as.factor(MultipleLines),
InternetService = as.factor(InternetService),
OnlineSecurity = as.factor(OnlineSecurity),
OnlineBackup = as.factor(OnlineBackup),
DeviceProtection = as.factor(DeviceProtection),
TechSupport = as.factor(TechSupport),
StreamingTV = as.factor(StreamingTV),
StreamingMovies = as.factor(StreamingMovies),
Contract = as.factor(Contract),
PaperlessBilling = as.factor(PaperlessBilling),
PaymentMethod = as.factor(PaymentMethod),
Churn = as.factor(Churn)
)
# Manejar valores nulos
# Eliminar filas con valores nulos en Contract
telco_data <- telco_data %>%
filter(!is.na(Contract))
# Completar valores nulos en gender con la moda
telco_data$gender[is.na(telco_data$gender)] <- telco_data %>%
count(gender) %>%
arrange(desc(n)) %>%
pull(gender) %>%
.[1]
# Completar valores nulos en MonthlyCharges con la media
telco_data$MonthlyCharges[is.na(telco_data$MonthlyCharges)] <- mean(telco_data$MonthlyCharges, na.rm = TRUE)
# Completar valores nulos en TotalCharges con la media
telco_data$TotalCharges[is.na(telco_data$TotalCharges)] <- mean(telco_data$TotalCharges, na.rm = TRUE)
# Contar el número de filas con valores nulos
total_na_rows <- sum(!complete.cases(telco_data))
cat("Número total de filas con valores nulos:", total_na_rows, "\n")
head(telco_data)
# Eliminar columna
telco_data <- telco_data %>% select(-customerID)
names(telco_data)
# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)  # Para reproducibilidad
train_index <- createDataPartition(telco_data$Churn, p = 0.7, list = FALSE)
train_data <- telco_data[train_index, ]
test_data <- telco_data[-train_index, ]
# Escalar los datos de entrenamiento
standard_scaler_train <- preProcess(train_data[, c("MonthlyCharges", "TotalCharges")], method = c("center", "scale"))
train_data_standard_scaled <- train_data
train_data_standard_scaled[, c("MonthlyCharges", "TotalCharges")] <- predict(standard_scaler_train, train_data[, c("MonthlyCharges", "TotalCharges")])
train_data_standard_scaled$MonthlyCharges
# Aplicar el escalador al conjunto de prueba utilizando los parámetros del conjunto de entrenamiento
test_data_standard_scaled <- test_data
test_data_standard_scaled[, c("MonthlyCharges", "TotalCharges")] <- predict(standard_scaler_train, test_data[, c("MonthlyCharges", "TotalCharges")])
test_data_standard_scaled$MonthlyCharges
# Escalar los datos de entrenamiento con MinMaxScaler
minmax_scaler_train <- preProcess(train_data[, c("MonthlyCharges", "TotalCharges")], method = c("range"))
train_data_minmax_scaled <- train_data
train_data_minmax_scaled[, c("MonthlyCharges", "TotalCharges")] <- predict(minmax_scaler_train, train_data[, c("MonthlyCharges", "TotalCharges")])
train_data_minmax_scaled$MonthlyCharges
# Aplicar el escalador min-max al conjunto de prueba
test_data_minmax_scaled <- test_data
test_data_minmax_scaled[, c("MonthlyCharges", "TotalCharges")] <- predict(minmax_scaler_train, test_data[, c("MonthlyCharges", "TotalCharges")])
test_data_minmax_scaled$MonthlyCharges
# Entrenar el árbol de decisión
decision_tree_model <- rpart(Churn ~ ., data = train_data_standard_scaled, method = "class")
# Predecir en el conjunto de prueba
dt_predictions <- predict(decision_tree_model, test_data_standard_scaled, type = "class")
# Evaluar el modelo
conf_matrix_tree <- confusionMatrix(dt_predictions, test_data$Churn)
conf_matrix_tree
accuracy_tree <- conf_matrix_tree$overall['Accuracy']
accuracy_tree
precision_tree <- conf_matrix_tree$byClass['Pos Pred Value']
recall_tree <- conf_matrix_tree$byClass['Sensitivity']
f1_score_tree <- 2 * (precision_tree * recall_tree) / (precision_tree + recall_tree)
# Entrenar el random forest
random_forest_model <- randomForest(Churn ~ ., data = train_data_minmax_scaled, ntree = 100)
# Predecir en el conjunto de prueba
rf_predictions <- predict(random_forest_model, test_data_minmax_scaled)
# Evaluar el modelo
conf_matrix_rf<-confusionMatrix(rf_predictions, test_data$Churn)
# Obtener Accuracy, Precision, Recall, F1 Score para Random Forest
accuracy_rf <- conf_matrix_rf$overall['Accuracy']
precision_rf <- conf_matrix_rf$byClass['Pos Pred Value']
recall_rf <- conf_matrix_rf$byClass['Sensitivity']
f1_score_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)
# Entrenar la regresión logística
logistic_model <- glm(Churn ~ ., data = train_data_minmax_scaled, family = binomial)
# Predecir en el conjunto de prueba
logistic_predictions <- predict(logistic_model, test_data_minmax_scaled, type = "response")
logistic_predictions_class <- ifelse(logistic_predictions > 0.5, "Yes", "No")
# Evaluar el modelo
conf_matrix_log<-confusionMatrix(as.factor(logistic_predictions_class), test_data$Churn)
# Obtener Accuracy, Precision, Recall, F1 Score para Regresión Logística
accuracy_log <- conf_matrix_log$overall['Accuracy']
precision_log <- conf_matrix_log$byClass['Pos Pred Value']
recall_log <- conf_matrix_log$byClass['Sensitivity']
f1_score_log <- 2 * (precision_log * recall_log) / (precision_log + recall_log)
# Entrenar la SVM
svm_model <- svm(Churn ~ ., data = train_data_standard_scaled, probability = TRUE)
# Predecir en el conjunto de prueba
svm_predictions <- predict(svm_model, test_data_standard_scaled, probability = TRUE)
# Evaluar el modelo
conf_matrix_svm<-confusionMatrix(svm_predictions, test_data$Churn)
# Obtener Accuracy, Precision, Recall, F1 Score para SVM
accuracy_svm <- conf_matrix_svm$overall['Accuracy']
precision_svm <- conf_matrix_svm$byClass['Pos Pred Value']
recall_svm <- conf_matrix_svm$byClass['Sensitivity']
f1_score_svm <- 2 * (precision_svm * recall_svm) / (precision_svm + recall_svm)
# Convertir variables categóricas a numéricas usando model.matrix
train_data_numeric <- model.matrix(Churn ~ . - 1, data = train_data_standard_scaled)
test_data_numeric <- model.matrix(Churn ~ . - 1, data = test_data_standard_scaled)
# Extraer la variable objetivo
train_labels <- train_data_standard_scaled$Churn
test_labels <- test_data_standard_scaled$Churn
# Ejecutar KNN
knn_predictions_standard <- knn(
train = train_data_numeric,
test = test_data_numeric,
cl = train_labels,
k = 5
)
# Evaluar el modelo
conf_matrix_knn<-confusionMatrix(knn_predictions_standard, test_labels)
# Obtener Accuracy, Precision, Recall, F1 Score para KNN
accuracy_knn <- conf_matrix_knn$overall['Accuracy']
precision_knn <- conf_matrix_knn$byClass['Pos Pred Value']
recall_knn <- conf_matrix_knn$byClass['Sensitivity']
f1_score_knn <- 2 * (precision_knn * recall_knn) / (precision_knn + recall_knn)
# Crear una tabla con las métricas
metrics <- tibble(
Model = c("Decision Tree", "Random Forest", "Logistic Regression", "SVM", "KNN"),
Accuracy = c(accuracy_tree, accuracy_rf, accuracy_log, accuracy_svm, accuracy_knn),
Precision = c(precision_tree, precision_rf, precision_log, precision_svm, precision_knn),
Recall = c(recall_tree, recall_rf, recall_log, recall_svm, recall_knn),
F1_Score = c(f1_score_tree, f1_score_rf, f1_score_log, f1_score_svm, f1_score_knn)
)
print(metrics)
# Mostrar la matriz de confusión para cada modelo
print("Matriz de Confusión Árbol de Decisión")
print(conf_matrix_tree$table)
print("Matriz de Confusión Random Forest")
print(conf_matrix_rf$table)
print("Matriz de Confusión Regresión Logística")
print(conf_matrix_log$table)
print("Matriz de Confusión SVM")
print(conf_matrix_svm$table)
print("Matriz de Confusión KNN")
print(conf_matrix_knn$table)
# Configuración de control de entrenamiento con validación cruzada de 10 pliegues
control <- trainControl(method = "cv", number = 10)
# Definir la rejilla de hiperparámetros para la búsqueda
tune_grid <- expand.grid(
mtry = c(2, 4, 6, 8, 10),
splitrule = "gini",
min.node.size = c(1, 5, 10)
)
# Entrenar el modelo Random Forest con validación cruzada y búsqueda de hiperparámetros
set.seed(123)
rf_model <- train(
Churn ~ .,
data = train_data_standard_scaled,
method = "ranger",
trControl = control,
tuneGrid = tune_grid,
metric = "Accuracy"
)
# Ver los mejores hiperparámetros
best_params <- rf_model$bestTune
print(best_params)
# Predecir en el conjunto de prueba
rf_predictions <- predict(rf_model, test_data)
# Evaluación
conf_matrix_rf <- confusionMatrix(rf_predictions, test_data$Churn)
print(conf_matrix_rf)
# Cargar el paquete MASS para acceder al conjunto de datos
library(MASS)
# Cargar el conjunto de datos de Diabetes de Pima Indians
data(Pima.tr)
# Ver la estructura del conjunto de datos
str(Pima.tr)
# Entrenar una regresión lineal múltiple para predecir Glucose
lm_model <- lm(glu ~ ., data = Pima.tr)
# Ver el resumen del modelo
summary(lm_model)
# Graficar los residuos
plot(lm_model, which = 1)
# Graficar los residuos
plot(lm_model, which = 1)
# Cargar el paquete para árboles de decisión
library(rpart)
# Entrenar un árbol de decisión de regresión
tree_model <- rpart(glu ~ ., data = Pima.tr)
# Ver el árbol creado
print(tree_model)
# Graficar el árbol
plot(tree_model)
text(tree_model, pretty = 0.5)
# Cargar el paquete para bosques aleatorios
library(randomForest)
# Entrenar un bosque aleatorio de regresión
rf_model <- randomForest(glu ~ ., data = Pima.tr)
# Ver el resumen del modelo
print(rf_model)
# Graficar la importancia de las variables
varImpPlot(rf_model)
install.packages(c("glmnet", "Metrics"))
library(glmnet)
library(Metrics)
# Ajustar un modelo Ridge
ridge_model <- glmnet(as.matrix(Pima.tr[, -ncol(Pima.tr)]), Pima.tr$glu, alpha = 0)
# Calcular predicciones del modelo
ridge_pred <- predict(ridge_model, newx = as.matrix(Pima.tr[, -ncol(Pima.tr)]))
# Calcular MSE y R²
ridge_mse <- mse(Pima.tr$glu, ridge_pred)
# Calcular R² para Ridge
ridge_r2 <- 1 - sum((Pima.tr$glu - ridge_pred)^2) / sum((Pima.tr$glu - mean(Pima.tr$glu))^2)
# Mostrar resultados
cat("Ridge Regression:\n")
cat("MSE:", ridge_mse, "\n")
cat("R²:", ridge_r2, "\n")
# Ajustar un modelo Lasso
lasso_model <- glmnet(as.matrix(Pima.tr[, -ncol(Pima.tr)]), Pima.tr$glu, alpha = 1)
# Calcular predicciones del modelo
lasso_pred <- predict(lasso_model, newx = as.matrix(Pima.tr[, -ncol(Pima.tr)]))
# Calcular MSE y R²
lasso_mse <- mse(Pima.tr$glu, lasso_pred)
lasso_r2 <- 1 - sum((Pima.tr$glu - lasso_pred)^2) / sum((Pima.tr$glu - mean(Pima.tr$glu))^2)
# Mostrar resultados
cat("\nLasso Regression:\n")
cat("MSE:", lasso_mse, "\n")
cat("R²:", lasso_r2, "\n")
