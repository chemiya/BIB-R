print(error.rate)
# Crear dataframe con los resultados
library(ggplot2)
k.values <- 1:20
error.df <- data.frame(error.rate,k.values)
error.df
# Gráfico para mostrar tasa de error
ggplot(error.df,aes(x=k.values,y=error.rate)) + geom_point()+ geom_line(lty="dotted",color='red')
library(datasets)
head(iris)
library(ggplot2)
# Gráfico de puntos
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
set.seed(101)
# Aplicamos kmeans
irisCluster <- kmeans(iris[, 1:4], 3, nstart = 20)
irisCluster
# Tabla con clusters
table(irisCluster$cluster, iris$Species)
irisCluster
library(cluster)
# Gráfico con los clusteres
clusplot(iris, irisCluster$cluster, color=TRUE, shade=TRUE, labels=0,lines=0, )
df <- read.csv('student-mat.csv',sep=';')
head(df)
summary(df)
any(is.na(df))
str(df)
library(ggplot2)
library(ggthemes)
library(dplyr)
# Coger solo columnas numéricas
num.cols <- sapply(df, is.numeric)
num.cols
# Filtrar columnas numéricas y ver correlacción
cor.data <- cor(df[,num.cols])
cor.data
library(corrplot)
library(corrgram)
# Diagrama de correlacción
corrplot(cor.data,method='color')
# Otro estilo, con gráficos circulares
corrgram(df,order=TRUE, lower.panel=panel.shade,
upper.panel=panel.pie, text.panel=panel.txt)
# Histograma para G3
ggplot(df,aes(x=G3)) + geom_histogram(bins=20,alpha=0.5,fill='blue') + theme_minimal()
library(caTools)
set.seed(101)
# Para separar en dos conjuntos
sample <- sample.split(df$age, SplitRatio = 0.70)
# Conjunto de entrenamiento
train = subset(df, sample == TRUE)
# Conjunto de test
test = subset(df, sample == FALSE)
# Modelo con regresión lineal
model <- lm(G3 ~ .,train)
summary(model)
# Ver residuos
res <- residuals(model)
# Convertir a dataframe
res <- as.data.frame(res)
head(res)
# Gráfico con los residuos
ggplot(res,aes(res)) +  geom_histogram(fill='blue',alpha=0.5)
# Ver campos del modelo
plot(model)
# Juntar predicciones y originales
results <- cbind(G3.predictions,test$G3)
# Cargar datos
df.train <- read.csv('titanic_train.csv')
head(df.train)
# Gráfico con los valores faltantes
library(Amelia)
# Gráfico con los valores faltantes
install.packages("Amelia")
library(Amelia)
missmap(df.train, main="Titanic Training Data - Missings Map",
col=c("yellow", "black"), legend=FALSE)
library(ggplot2)
ggplot(df.train,aes(Survived)) + geom_bar()
ggplot(df.train,aes(Pclass)) + geom_bar(aes(fill=factor(Pclass)),alpha=0.5)
ggplot(df.train,aes(Sex)) + geom_bar(aes(fill=factor(Sex)),alpha=0.5)
ggplot(df.train,aes(Age)) + geom_histogram(fill='blue',bins=20,alpha=0.5)
ggplot(df.train,aes(SibSp)) + geom_bar(fill='red',alpha=0.5)
ggplot(df.train,aes(Fare)) + geom_histogram(fill='green',color='black',alpha=0.5)
pl <- ggplot(df.train,aes(Pclass,Age)) + geom_boxplot(aes(group=Pclass,fill=factor(Pclass),alpha=0.4))
pl + scale_y_continuous(breaks = seq(min(0), max(80), by = 2))
# Completar valores faltantes de la edad
impute_age <- function(age,class){
out <- age
for (i in 1:length(age)){
if (is.na(age[i])){
if (class[i] == 1){
out[i] <- 37
}else if (class[i] == 2){
out[i] <- 29
}else{
out[i] <- 24
}
}else{
out[i]<-age[i]
}
}
return(out)
}
# Completamos los valores
fixed.ages <- impute_age(df.train$Age,df.train$Pclass)
df.train$Age <- fixed.ages
missmap(df.train, main="Titanic Training Data - Missings Map",
col=c("yellow", "black"), legend=FALSE)
str(df.train)
head(df.train,3)
library(dplyr)
df.train <- select(df.train,-PassengerId,-Name,-Ticket,-Cabin)
head(df.train,3)
# Se convierte a factores
df.train$Survived <- factor(df.train$Survived)
df.train$Pclass <- factor(df.train$Pclass)
df.train$Parch <- factor(df.train$Parch)
df.train$SibSp <- factor(df.train$SibSp)
# Regresión logística
log.model <- glm(formula=Survived ~ . , family = binomial(link='logit'),data = df.train)
summary(log.model)
library(caTools)
set.seed(101)
# Para separar en dos conjuntos
split = sample.split(df.train$Survived, SplitRatio = 0.70)
final.train = subset(df.train, split == TRUE)
final.test = subset(df.train, split == FALSE)
final.log.model <- glm(formula=Survived ~ . , family = binomial(link='logit'),data = final.train)
summary(final.log.model)
# Calcular probabilidades sobre el conjunto de test
fitted.probabilities <- predict(final.log.model,newdata=final.test,type='response')
fitted.probabilities
fitted.results <- ifelse(fitted.probabilities > 0.5,1,0)
fitted.results
misClasificError <- mean(fitted.results != final.test$Survived)
print(paste('Accuracy',1-misClasificError))
table(final.test$Survived, fitted.probabilities > 0.5)
# Separa por el guión
strsplit('2016-01-23',split='-')
substr('abcdefg',start=2,stop = 5)
print(paste('A','B','C',sep='...'))
gsub('pattern','replacement','hello have you seen the pattern here?')
nchar('hello world')
grep('A', c('A','B','C','D','A'))
library(MASS)
set.seed(101)
data <- Boston
str(data)
summary(data)
head(data)
any(is.na(data))
library(neuralnet)
install.packages("neuralnet")
library(neuralnet)
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
maxs
mins
head(data)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
head(scaled)
library(caTools)
split = sample.split(scaled$medv, SplitRatio = 0.70)
n <- names(train)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
f
# Crea la red neuronal
nn <- neuralnet(f,data=train,hidden=c(5,3),linear.output=TRUE)
# Crea la red neuronal
nn <- neuralnet(f,data=train,hidden=c(5,3),linear.output=TRUE)
train
train = subset(scaled, split == TRUE)
test = subset(scaled, split == FALSE)
train
# Crea la red neuronal
nn <- neuralnet(f,data=train,hidden=c(5,3),linear.output=TRUE)
library(MASS)
set.seed(101)
data <- Boston
str(data)
summary(data)
head(data)
any(is.na(data))
install.packages("neuralnet")
install.packages("neuralnet")
library(neuralnet)
# Encuentra el máximo y mínimo de cada columna
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
maxs
mins
# Escala los datos
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
head(scaled)
# Separa los datos en dos conjuntos
library(caTools)
split = sample.split(scaled$medv, SplitRatio = 0.70)
train = subset(scaled, split == TRUE)
test = subset(scaled, split == FALSE)
# Prepara los nombres
n <- names(train)
n
# Concatena los nombres
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
f
# Crea la red neuronal
nn <- neuralnet(f,data=train,hidden=c(5,3),linear.output=TRUE)
predicted.nn.values <- compute(nn,test[1:13])
str(predicted.nn.values)
true.predictions <- predicted.nn.values$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
true.predictions
# También para las muestras de test
test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - true.predictions)^2)/nrow(test)
MSE.nn
error.df <- data.frame(test.r,true.predictions)
head(error.df)
library(ggplot2)
ggplot(error.df,aes(x=test.r,y=true.predictions)) + geom_point() + stat_smooth()
library(ISLR)
head(iris)
library(e1071)
model <- svm(Species ~ ., data=iris)
summary(model)
# Realizar predicciones
predicted.values <- predict(model,iris[1:4])
predicted.values
table(predicted.values,iris[,5])
tune.results <- tune(svm,train.x=iris[1:4],train.y=iris[,5],kernel='radial',
ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
summary(tune.results)
# Modelo con mejores parámetros
tuned.svm <- svm(Species ~ ., data=iris, kernel="radial", cost=1, gamma=0.5)
summary(tuned.svm)
# Predicciones
tuned.predicted.values <- predict(tuned.svm,iris[1:4])
table(tuned.predicted.values,iris[,5])
# Leer datos
batting <- read.csv('Batting.csv')
head(batting)
str(batting)
# Crear nuevos atributos
batting$BA <- batting$H / batting$AB
batting$OBP <- (batting$H + batting$BB + batting$HBP)/(batting$AB + batting$BB + batting$HBP + batting$SF)
batting$X1B <- batting$H - batting$X2B - batting$X3B - batting$HR
batting$SLG <- ((1 * batting$X1B) + (2 * batting$X2B) + (3 * batting$X3B) + (4 * batting$HR) ) / batting$AB
# Leer salarios
sal <- read.csv('Salaries.csv')
# Filtrar
batting <- subset(batting,yearID >= 1985)
# Juntar datos
combo <- merge(batting,sal,by=c('playerID','yearID'))
summary(combo)
combo
# Filtrar jugadores
lost_players <- subset(combo,playerID %in% c('giambja01','damonjo01','saenzol01') )
lost_players
# Filtrar por año
lost_players <- subset(lost_players,yearID == 2001)
# Seleccionar columnas
lost_players <- lost_players[,c('playerID','H','X2B','X3B','HR','OBP','SLG','BA','AB')]
head(lost_players)
library(dplyr)
avail.players <- filter(combo,yearID==2001)
library(ggplot2)
ggplot(avail.players,aes(x=OBP,y=salary)) + geom_point()
# Filtrar por condiciones
avail.players <- filter(avail.players,salary<8000000,OBP>0)
avail.players <- filter(avail.players,AB >= 500)
possible <- head(arrange(avail.players,desc(OBP)),10)
possible
# Selecciona columnas
possible <- possible[,c('playerID','OBP','AB','salary')]
possible
possible[2:4,]
library(ggplot2)
library(data.table)
df <- fread('Economist_Assignment_Data.csv',drop=1)
head(df)
pl <- ggplot(df,aes(x=CPI,y=HDI,color=Region)) + geom_point()
pl
pl <- ggplot(df,aes(x=CPI,y=HDI,color=Region)) + geom_point(size=4,shape=1)
pl
pl + geom_smooth(aes(group=1))
pl2 <- pl + geom_smooth(aes(group=1),method ='lm',formula = y~log(x),se=FALSE,color='red')
pl2
pl2 + geom_text(aes(label=Country))
pointsToLabel <- c("Russia", "Venezuela", "Iraq", "Myanmar", "Sudan",
"Afghanistan", "Congo", "Greece", "Argentina", "Brazil",
"India", "Italy", "China", "South Africa", "Spane",
"Botswana", "Cape Verde", "Bhutan", "Rwanda", "France",
"United States", "Germany", "Britain", "Barbados", "Norway", "Japan",
"New Zealand", "Singapore")
pl3 <- pl2 + geom_text(aes(label = Country), color = "gray20",
data = subset(df, Country %in% pointsToLabel),check_overlap = TRUE)
pl3
pl4 <- pl3 + theme_bw()
pl4
pl5 <- pl4 + scale_x_continuous(name = "Corruption Perceptions Index, 2011 (10=least corrupt)",
limits = c(.9, 10.5),breaks=1:10)
pl5
# Eje y modificar
pl6 <- pl5 + scale_y_continuous(name = "Human Development Index, 2011 (1=Best)",
limits = c(0.2, 1.0))
pl6
# Título
pl6 + ggtitle("Corruption and Human development")
library(ggthemes)
pl6 + theme_economist_white()
library(dplyr)
head(mtcars)
filter(mtcars,mpg>20,cyl==6)
head(arrange(mtcars,cyl,desc(wt)))
distinct(select(mtcars,gear))
head(select(mtcars,mpg,hp))
head(mutate(mtcars,Performance=hp/wt))
summarise(mtcars,avg_mpg=mean(mpg))
mtcars %>% filter(cyl==6) %>% summarise(avg_hp = mean(hp))
# Leer datos
df1 <- read.csv('winequality-red.csv',sep=';')
df2 <- read.csv('winequality-white.csv',sep=';')
# Poner etiquetas identificativas
df1$label <- sapply(df1$pH,function(x){'red'})
df2$label <- sapply(df2$pH,function(x){'white'})
# Unir datos
wine <- rbind(df1,df2)
library(ggplot2)
pl <- ggplot(wine,aes(x=residual.sugar)) + geom_histogram(aes(fill=label),color='black',bins=50)
pl + scale_fill_manual(values = c('#ae4554','#faf7ea')) + theme_bw()
pl <- ggplot(wine,aes(x=citric.acid)) + geom_histogram(aes(fill=label),color='black',bins=50)
pl + scale_fill_manual(values = c('#ae4554','#faf7ea')) + theme_bw()
pl <- ggplot(wine,aes(x=alcohol)) + geom_histogram(aes(fill=label),color='black',bins=50)
pl + scale_fill_manual(values = c('#ae4554','#faf7ea')) + theme_bw()
pl <- ggplot(wine,aes(x=citric.acid,y=residual.sugar)) + geom_point(aes(color=label),alpha=0.2)
pl + scale_color_manual(values = c('#ae4554','#faf7ea')) +theme_dark()
pl <- ggplot(wine,aes(x=volatile.acidity,y=residual.sugar)) + geom_point(aes(color=label),alpha=0.2)
pl + scale_color_manual(values = c('#ae4554','#faf7ea')) +theme_dark()
clus.data <- wine[,1:12]
wine.cluster <- kmeans(wine[1:12],2)
print(wine.cluster$centers)
table(wine$label,wine.cluster$cluster)
library(ISLR)
head(iris)
stand.features <- scale(iris[1:4])
var(stand.features[,1])
# Unir datos
final.data <- cbind(stand.features,iris[5])
set.seed(101)
library(caTools)
# Separar en conjuntos
sample <- sample.split(final.data$Species, SplitRatio = .70)
train <- subset(final.data, sample == TRUE)
test <- subset(final.data, sample == FALSE)
# Kvecinos
predicted.species <- knn(train[1:4],test[1:4],train$Species,k=1)
predicted.species
mean(test$Species != predicted.species)
predicted.species <- NULL
error.rate <- NULL
# Incrementar el número de vecinos
for(i in 1:10){
set.seed(101)
predicted.species <- knn(train[1:4],test[1:4],train$Species,k=i)
error.rate[i] <- mean(test$Species != predicted.species)
}
library(ggplot2)
k.values <- 1:10
error.df <- data.frame(error.rate,k.values)
pl <- ggplot(error.df,aes(x=k.values,y=error.rate)) + geom_point()
pl + geom_line(lty="dotted",color='red')
# Leer datos
bike <- read.csv('bikeshare.csv')
library(ggplot2)
ggplot(bike,aes(temp,count)) + geom_point(alpha=0.2, aes(color=temp)) + theme_bw()
bike$datetime <- as.POSIXct(bike$datetime)
ggplot(bike,aes(datetime,count)) + geom_point(aes(color=temp),alpha=0.5)  + scale_color_continuous(low='#55D8CE',high='#FF6E2E') +theme_bw()
cor(bike[,c('temp','count')])
ggplot(bike,aes(factor(season),count)) + geom_boxplot(aes(color=factor(season))) +theme_bw()
bike$hour <- sapply(bike$datetime,function(x){format(x,"%H")})
bike$hour
library(dplyr)
pl <- ggplot(filter(bike,workingday==1),aes(hour,count))
pl <- pl + geom_point(position=position_jitter(w=1, h=0),aes(color=temp),alpha=0.5)
pl <- pl + scale_color_gradientn(colours = c('dark blue','blue','light blue','light green','yellow','orange','red'))
pl + theme_bw()
pl <- ggplot(filter(bike,workingday==0),aes(hour,count))
pl <- pl + geom_point(position=position_jitter(w=1, h=0),aes(color=temp),alpha=0.8)
pl <- pl + scale_color_gradientn(colours = c('dark blue','blue','light blue','light green','yellow','orange','red'))
pl + theme_bw()
# Regresión lineal
temp.model <- lm(count~temp,bike)
summary(temp.model)
temp.test <- data.frame(temp=c(25))
temp.test
predict(temp.model,temp.test)
bike$hour <- sapply(bike$hour,as.numeric)
model <- lm(count ~ . -casual - registered -datetime -atemp,bike )
summary(model)
df <- read.csv('bank_note_data.csv')
head(df)
library(caTools)
set.seed(101)
split = sample.split(df$Class, SplitRatio = 0.70)
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)
library(neuralnet)
nn <- neuralnet(Class ~ Image.Var + Image.Skew + Image.Curt + Entropy,data=train,hidden=10,linear.output=FALSE)
predicted.nn.values <- compute(nn,test[,1:4])
head(predicted.nn.values$net.result)
# Redondear predicciones
predictions <- sapply(predicted.nn.values$net.result,round)
table(predictions,test$Class)
library(randomForest)
df$Class <- factor(df$Class)
library(caTools)
set.seed(101)
# Separar datos
split = sample.split(df$Class, SplitRatio = 0.70)
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)
# Realizar predicciones
rf.pred <- predict(model,test)
# Bosque aleatorio
model <- randomForest(Class ~ Image.Var + Image.Skew + Image.Curt + Entropy,data=train)
# Realizar predicciones
rf.pred <- predict(model,test)
# Matriz de confusión
table(rf.pred,test$Class)
library(ISLR)
head(College)
df<-College
library(ggplot2)
ggplot(df,aes(Room.Board,Grad.Rate)) + geom_point(aes(color=Private))
ggplot(df,aes(F.Undergrad)) + geom_histogram(aes(fill=Private),color='black',bins=50)
ggplot(df,aes(Grad.Rate)) + geom_histogram(aes(fill=Private),color='black',bins=50)
# Seleccionar datos
subset(df,Grad.Rate > 100)
# Ajustar dato
df['Cazenovia College','Grad.Rate'] <- 100
# Separar datos
library(caTools)
set.seed(101)
sample = sample.split(df$Private, SplitRatio = .70)
train = subset(df, sample == TRUE)
test = subset(df, sample == FALSE)
library(rpart)
tree <- rpart(Private ~.,method='class',data = train)
tree.preds <- predict(tree,test)
head(tree.preds)
tree.preds <- as.data.frame(tree.preds)
# Árbol de decisión
library(rpart)
tree <- rpart(Private ~.,method='class',data = train)
tree.preds <- predict(tree,test)
head(tree.preds)
# Comprobar superior a 0,5
joiner <- function(x){
if (x>=0.5){
return('Yes')
}else{
return("No")
}
}
# Aplicar función
tree.preds$Private <- sapply(tree.preds$Yes,joiner)
library(ISLR)
head(College)
df<-College
library(ggplot2)
# De puntos
ggplot(df,aes(Room.Board,Grad.Rate)) + geom_point(aes(color=Private))
# Histograma
ggplot(df,aes(F.Undergrad)) + geom_histogram(aes(fill=Private),color='black',bins=50)
ggplot(df,aes(Grad.Rate)) + geom_histogram(aes(fill=Private),color='black',bins=50)
# Seleccionar datos
subset(df,Grad.Rate > 100)
# Ajustar dato
df['Cazenovia College','Grad.Rate'] <- 100
# Separar datos
library(caTools)
set.seed(101)
sample = sample.split(df$Private, SplitRatio = .70)
train = subset(df, sample == TRUE)
test = subset(df, sample == FALSE)
# Árbol de decisión
library(rpart)
tree <- rpart(Private ~.,method='class',data = train)
tree.preds <- predict(tree,test)
head(tree.preds)
tree.preds <- as.data.frame(tree.preds)
# Comprobar superior a 0,5
joiner <- function(x){
if (x>=0.5){
return('Yes')
}else{
return("No")
}
}
# Aplicar función
tree.preds$Private <- sapply(tree.preds$Yes,joiner)
head(tree.preds)
table(tree.preds$Private,test$Private)
library(rpart.plot)
prp(tree)
library(randomForest)
rf.model <- randomForest(Private ~ . , data = train,importance = TRUE)
rf.model$confusion
rf.model$importance
p <- predict(rf.model,test)
table(p,test$Private)
